{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d23931-82d9-49e9-8a08-71a3497cda77",
   "metadata": {},
   "source": [
    "This notebook is an example of the complete pipeline of the classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578c906-efec-48d5-b0a9-494effe1e236",
   "metadata": {},
   "source": [
    "Data used for this example is extracted from: https://zenodo.org/record/3520150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb562155-f031-4ccc-bed4-1072c9ed707a",
   "metadata": {},
   "source": [
    "The code is based on the tutorial found in: https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c9c85-d7c4-42bd-a48e-a51e649fde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a24b4-baeb-4bff-921f-d11b4c29e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, AutoModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c14450-13b1-4e76-b106-64403aa1d7d8",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faeee1c-d5b3-4693-900c-a9c186b77ec1",
   "metadata": {},
   "source": [
    "## Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88a4b7-ca46-4d57-a8e4-2d886c63085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/dachs/Train_Hate_Messages.csv\"\n",
    "TEST_FILE = \"data/dachs/Test_Hate_Messages.csv\"\n",
    "SEED_SHUFFLE = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f523cd-5d79-47ab-b038-4ec40522bc05",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c571732-d4d0-455a-9e80-1d61c70303b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.3\n",
    "SEED_TRAIN_VALIDATION_SEPARATION = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6dc41-54af-4907-abe2-3f8cea847d27",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdac6d-0481-4b11-b77b-716c6b46d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "OUTPUT_FILE = \"models/dachs/best_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1edb7b-5cb2-45a2-95ea-751fb6aac71a",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452445b-6352-4fca-9e84-890023afd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55f838-6de1-415f-8056-36f683fe4ab9",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f1e20-b3e0-40ef-ada4-db4ca1a4bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581d444-46a4-476e-86c5-8bddd693082e",
   "metadata": {},
   "source": [
    "## Read data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191e332-b1f5-4e41-910f-c268385cc5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_dachs_messages(data_file):\n",
    "    df = pd.read_csv(data_file, sep=\"|\")\n",
    "    df = df.dropna()\n",
    "    df = df.rename(columns={\"Hate_Speech\": \"label\"})\n",
    "    df = df[[\"message\", \"label\"]]\n",
    "    return df.sample(frac=1, random_state=SEED_SHUFFLE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288146d3-a785-4c14-b884-2c3897ffbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_data_dachs_messages(TRAIN_FILE)\n",
    "print(len(train_df[\"label\"]))\n",
    "print(train_df[\"label\"].value_counts())\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32eea7-cff1-443b-aa35-213d2dee966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_data_dachs_messages(TEST_FILE)\n",
    "print(len(test_df))\n",
    "print(test_df[\"label\"].value_counts())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8439fb-3ae8-4433-8753-7e0aa0010317",
   "metadata": {},
   "source": [
    "## Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, validation_messages, train_labels, validation_labels = train_test_split(train_df[\"message\"], train_df[\"label\"],\n",
    "                                                                                        random_state=SEED_TRAIN_VALIDATION_SEPARATION,\n",
    "                                                                                        test_size=VALIDATION_SIZE,\n",
    "                                                                                        stratify=train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c3539-6f57-40c4-8826-b82cdbca7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = test_df[\"message\"]\n",
    "test_labels = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f185e-b1b9-46d0-8f6e-ffe1aa38d5e0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1ea8c-3448-4d86-b213-6364075dc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_messages(messages):\n",
    "    return tokenizer.batch_encode_plus(messages.tolist(),\n",
    "                                       max_length = SEQUENCE_LENGTH,\n",
    "                                       padding=\"max_length\",\n",
    "                                       truncation=True,\n",
    "                                       add_special_tokens=True,\n",
    "                                       return_token_type_ids=False)\n",
    "\n",
    "def tokenize_and_create_tensors(messages, labels):\n",
    "    tokens = tokenize_messages(messages)\n",
    "    return {\"token_ids\": torch.tensor(tokens[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(tokens[\"attention_mask\"]),\n",
    "            \"labels\": torch.tensor(labels.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32683b9-f667-4a44-aa36-27a50b1a7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = tokenize_and_create_tensors(train_messages, train_labels)\n",
    "validation_tensors = tokenize_and_create_tensors(validation_messages, validation_labels)\n",
    "test_tensors = tokenize_and_create_tensors(test_messages, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5503de-ace6-4647-9bfe-398c5a5da011",
   "metadata": {},
   "source": [
    "## Dataset, Sampler and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6ad9e-c176-4a00-862b-bf102aa8b19e",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dff6d1-d6f7-44d2-8f0a-80de82a77119",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tensors[\"token_ids\"], train_tensors[\"attention_mask\"], train_tensors[\"labels\"])\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60b5aa-9661-4884-a10e-6a04a0a6dc7f",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = TensorDataset(validation_tensors[\"token_ids\"], validation_tensors[\"attention_mask\"], validation_tensors[\"labels\"])\n",
    "validation_sampler = SequentialSampler(validation_dataset)\n",
    "validation_dataloader = DataLoader(validation_dataset, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624b646-28fd-46d5-8c94-078efb2dfe3c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c4b15-f0c8-41ea-8ac3-2611153c7225",
   "metadata": {},
   "source": [
    "## Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModel.from_pretrained(BERT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42317132-bb8c-49c3-8932-50a652d6aca5",
   "metadata": {},
   "source": [
    "### Freeze all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18d38b-ca6e-4b8a-8ad1-1bcb8d91fa83",
   "metadata": {},
   "source": [
    "## Custom classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.bert = bert \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu =  nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)[\"pooler_output\"]\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierModel(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a5f7f-94c1-453b-a939-21de58298f03",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0bd45-8eac-42ba-bd5c-42187f0bec46",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076eaced-f2a6-43dd-880e-a6bc8a10cdf1",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\"balanced\", np.unique(train_labels), train_labels)\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "weights = weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.NLLLoss(weight=weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b38e44-fb1b-4bd3-b0a8-61fb77eeaadf",
   "metadata": {},
   "source": [
    "### Train and evaluate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_preds=[]\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(tqdm(validation_dataloader)):\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "            \n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(validation_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a772add-14d0-4673-ad74-08a3f04ada6d",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-pipeline",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "     \n",
    "    print(\"\\n Epoch {:} / {:}\".format(epoch + 1, EPOCHS))\n",
    "\n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), OUTPUT_FILE)\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f\"\\nTraining Loss: {train_loss:.3f}\")\n",
    "    print(f\"Validation Loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefe236-f6c1-46f7-8c93-9617fcce706b",
   "metadata": {},
   "source": [
    "## Performance report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(OUTPUT_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-enhancement",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = model(test_tensors[\"token_ids\"][0:1000].to(DEVICE), test_tensors[\"attention_mask\"][0:1000].to(DEVICE))\n",
    "    predictions = predictions.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis = 1)\n",
    "print(classification_report(test_tensors[\"labels\"][0:1000], predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
